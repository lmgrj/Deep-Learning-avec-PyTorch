{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lmgrj/Deep-Learning-avec-PyTorch/blob/main/TP_N%C2%B0_3_DEEP_LEARNING.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><h1>WISD&EID2/Deep Learning</center></h1> \n",
        "\n",
        "<center><h1>TP 3 </center><h1>\n",
        "\n",
        "<center><h1>Adaline , Perceptron and Multi layer Perceptron</center><h1>\n",
        "\n",
        "\n",
        "---\n",
        "By: **Lamgarraj Mohamed**   &   **Yazid Tagnaouti Moumnani**\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "HX5xL2kgUdjj"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UkMZz4wA38zj"
      },
      "source": [
        "# Deep Learning\n",
        "## Practical Deep Learning Tutorial with PyTorch - Tutorial NÂ° 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFqfmTX638zn"
      },
      "source": [
        "### 2020-2021"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7K3YTmcv38zo"
      },
      "source": [
        "# Importing necessary libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fQrOOPut38zp"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.autograd import grad\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_blobs\n",
        "from torch import nn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MtFI4c1q38zq"
      },
      "source": [
        "# Adaline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKRrSWZK38zr"
      },
      "source": [
        "1. Built ADALINE model using the nn.Module class "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bSJ1IHf638zr"
      },
      "outputs": [],
      "source": [
        "class Adaline(torch.nn.Module):\n",
        "    def __init__(self, num_features):\n",
        "        super(Adaline, self).__init__()\n",
        "        self.linear = nn.Linear(num_features,1)                                                                                                       \n",
        "        self.linear.weight.detach().zero_()            \n",
        "        self.linear.bias.detach().zero_()              \n",
        "    \n",
        "    def forward(self, x):\n",
        "        activations = self.linear(x)\n",
        "        return activations.view(-1)\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nt2pDaVD38zr"
      },
      "source": [
        "2. Using 'iris.txt', create a binary datasets in 2-D : The last 100 instances of iris described only by the 2nd and 3rd features\n",
        "    \n",
        "    Split the dataset into traing and test sets (70%,30%) \n",
        "\n",
        "    Normalize the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UzWrHZHr38zs",
        "outputId": "668fcc31-dedb-4800-d81b-b4006b6ac3ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "path='/content/drive/My Drive/deeplearning/iris.txt'\n",
        "\n",
        "df = pd.read_csv(path, index_col=None, header=None)    #reding iris\n",
        "df.columns = ['x1', 'x2', 'x3', 'x4', 'y'] #define features names\n",
        "df = df.iloc[50:150]   #taking last 100 elements that belong to 2 classes (100*2)\n",
        "df['y'] = df['y'].apply(lambda x: 0 if x == 'Iris-versicolor' else 1) #coder la 2e classe par 0 la 3e par 1\n",
        "\n",
        "\n",
        "# Assign features and target\n",
        "\n",
        "X = torch.tensor(df[['x2', 'x3']].values, dtype=torch.float) #taking just last 2 features\n",
        "y = torch.tensor(df['y'].values, dtype=torch.int) # y is a tensor of features\n",
        "\n",
        "# Shuffling & train/test split\n",
        "\n",
        "torch.manual_seed(123)\n",
        "shuffle_idx = torch.randperm(y.size(0), dtype=torch.long)  #shuffle indexes\n",
        "X, y = X[shuffle_idx], y[shuffle_idx]  #shuffle data (we shuffle also labels)\n",
        "percent70 = int(shuffle_idx.size(0)*0.7)\n",
        "X_train, X_test = X[shuffle_idx[:percent70]], X[shuffle_idx[percent70:]]  # 70% for training training \n",
        "y_train, y_test = y[shuffle_idx[:percent70]], y[shuffle_idx[percent70:]]  # 30% for  testing\n",
        "\n",
        "# Normalize (mean zero, unit variance)\n",
        "\n",
        "mu, sigma = X_train.mean(dim=0), X_train.std(dim=0)\n",
        "X_train = (X_train - mu )/sigma\n",
        "X_test = (X_test - mu )/sigma"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLNIz5Ow38zs"
      },
      "source": [
        "3. Train the model : we will use MSELoss (mean squared error (squared L2 norm)) as loss function. The optimizer is SGD (Stochastic Gradient Descent) with learning rate 0.01."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pp4X5lJB38zt"
      },
      "outputs": [],
      "source": [
        "def train(model, x, y, epochs, learning_rate, seed):\n",
        "    cost = []\n",
        "    torch.manual_seed(seed)\n",
        "    optimizer = torch.optim.SGD(model.parameters(),lr=learning_rate)   #use a SGD optimizer\n",
        "    c = nn.MSELoss()\n",
        "    for e in range(epochs):\n",
        "        yhat = model.forward(x) # calcul yhat\n",
        "        loss = c(yhat,y) # calcul the loss function using MSE\n",
        "        optimizer.zero_grad()    # set the gradients to zero\n",
        "        loss.backward()  # calculer le gradients\n",
        "        optimizer.step()  # mise a jour des poids ####                        \n",
        "        print('epoch {} , loss {}'.format(e,loss.item()))  "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Adaline(num_features=X_train.size(1))\n",
        "train(model, X_train, y_train.float(),num_epochs=100,learning_rate=0.01,seed=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xDfjkGwWq9ja",
        "outputId": "84431d78-624c-40d1-da24-a3282a12a0e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0 , loss 0.48571428656578064\n",
            "epoch 1 , loss 0.4699629247188568\n",
            "epoch 2 , loss 0.45489606261253357\n",
            "epoch 3 , loss 0.4404822289943695\n",
            "epoch 4 , loss 0.42669153213500977\n",
            "epoch 5 , loss 0.4134954810142517\n",
            "epoch 6 , loss 0.40086689591407776\n",
            "epoch 7 , loss 0.3887799382209778\n",
            "epoch 8 , loss 0.37721002101898193\n",
            "epoch 9 , loss 0.3661336600780487\n",
            "epoch 10 , loss 0.35552868247032166\n",
            "epoch 11 , loss 0.3453736901283264\n",
            "epoch 12 , loss 0.33564847707748413\n",
            "epoch 13 , loss 0.3263336420059204\n",
            "epoch 14 , loss 0.317410945892334\n",
            "epoch 15 , loss 0.30886268615722656\n",
            "epoch 16 , loss 0.30067235231399536\n",
            "epoch 17 , loss 0.29282382130622864\n",
            "epoch 18 , loss 0.2853020429611206\n",
            "epoch 19 , loss 0.27809247374534607\n",
            "epoch 20 , loss 0.2711813747882843\n",
            "epoch 21 , loss 0.26455551385879517\n",
            "epoch 22 , loss 0.2582024037837982\n",
            "epoch 23 , loss 0.2521100640296936\n",
            "epoch 24 , loss 0.24626705050468445\n",
            "epoch 25 , loss 0.24066250026226044\n",
            "epoch 26 , loss 0.23528605699539185\n",
            "epoch 27 , loss 0.2301277220249176\n",
            "epoch 28 , loss 0.2251780778169632\n",
            "epoch 29 , loss 0.22042809426784515\n",
            "epoch 30 , loss 0.21586920320987701\n",
            "epoch 31 , loss 0.2114931344985962\n",
            "epoch 32 , loss 0.20729205012321472\n",
            "epoch 33 , loss 0.2032584398984909\n",
            "epoch 34 , loss 0.19938518106937408\n",
            "epoch 35 , loss 0.1956653892993927\n",
            "epoch 36 , loss 0.19209259748458862\n",
            "epoch 37 , loss 0.18866051733493805\n",
            "epoch 38 , loss 0.1853632628917694\n",
            "epoch 39 , loss 0.18219506740570068\n",
            "epoch 40 , loss 0.17915058135986328\n",
            "epoch 41 , loss 0.17622457444667816\n",
            "epoch 42 , loss 0.17341205477714539\n",
            "epoch 43 , loss 0.1707083284854889\n",
            "epoch 44 , loss 0.16810885071754456\n",
            "epoch 45 , loss 0.16560928523540497\n",
            "epoch 46 , loss 0.16320547461509705\n",
            "epoch 47 , loss 0.1608934998512268\n",
            "epoch 48 , loss 0.15866954624652863\n",
            "epoch 49 , loss 0.15653002262115479\n",
            "epoch 50 , loss 0.15447144210338593\n",
            "epoch 51 , loss 0.1524905264377594\n",
            "epoch 52 , loss 0.15058404207229614\n",
            "epoch 53 , loss 0.14874902367591858\n",
            "epoch 54 , loss 0.14698255062103271\n",
            "epoch 55 , loss 0.1452818512916565\n",
            "epoch 56 , loss 0.14364425837993622\n",
            "epoch 57 , loss 0.14206725358963013\n",
            "epoch 58 , loss 0.14054837822914124\n",
            "epoch 59 , loss 0.13908536732196808\n",
            "epoch 60 , loss 0.1376759111881256\n",
            "epoch 61 , loss 0.1363179087638855\n",
            "epoch 62 , loss 0.13500936329364777\n",
            "epoch 63 , loss 0.13374823331832886\n",
            "epoch 64 , loss 0.13253268599510193\n",
            "epoch 65 , loss 0.1313609629869461\n",
            "epoch 66 , loss 0.13023130595684052\n",
            "epoch 67 , loss 0.12914206087589264\n",
            "epoch 68 , loss 0.1280916929244995\n",
            "epoch 69 , loss 0.1270786076784134\n",
            "epoch 70 , loss 0.12610146403312683\n",
            "epoch 71 , loss 0.12515878677368164\n",
            "epoch 72 , loss 0.12424926459789276\n",
            "epoch 73 , loss 0.1233716681599617\n",
            "epoch 74 , loss 0.12252470850944519\n",
            "epoch 75 , loss 0.1217072606086731\n",
            "epoch 76 , loss 0.12091819196939468\n",
            "epoch 77 , loss 0.12015638500452042\n",
            "epoch 78 , loss 0.11942082643508911\n",
            "epoch 79 , loss 0.11871055513620377\n",
            "epoch 80 , loss 0.11802458018064499\n",
            "epoch 81 , loss 0.11736200749874115\n",
            "epoch 82 , loss 0.11672195792198181\n",
            "epoch 83 , loss 0.11610356718301773\n",
            "epoch 84 , loss 0.11550606042146683\n",
            "epoch 85 , loss 0.11492867022752762\n",
            "epoch 86 , loss 0.11437057703733444\n",
            "epoch 87 , loss 0.11383116245269775\n",
            "epoch 88 , loss 0.11330968886613846\n",
            "epoch 89 , loss 0.11280549317598343\n",
            "epoch 90 , loss 0.11231797188520432\n",
            "epoch 91 , loss 0.11184649169445038\n",
            "epoch 92 , loss 0.11139050871133804\n",
            "epoch 93 , loss 0.11094939708709717\n",
            "epoch 94 , loss 0.11052266508340836\n",
            "epoch 95 , loss 0.11010980606079102\n",
            "epoch 96 , loss 0.10971028357744217\n",
            "epoch 97 , loss 0.1093236431479454\n",
            "epoch 98 , loss 0.10894942283630371\n",
            "epoch 99 , loss 0.10858717560768127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mm2ft7J338zt"
      },
      "source": [
        "4. Compute the model accuracy "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VE2vS_Jh38zt",
        "outputId": "5057fdd4-7d67-4b20-ea94-7338d9657234"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Accuracy: 87.14\n",
            "Test Accuracy: 76.67\n",
            "Weights Parameter containing:\n",
            "tensor([[0.0043, 0.3283]], requires_grad=True)\n",
            "Bias Parameter containing:\n",
            "tensor([0.4213], requires_grad=True)\n"
          ]
        }
      ],
      "source": [
        "def custom_where(cond, x_1, x_2):\n",
        "    return (cond * x_1) + (torch.logical_not(cond) * x_2)\n",
        "train_pred = model.forward(X_train)\n",
        "train_acc = torch.mean(\n",
        "    (custom_where(train_pred > 0.5, 1, 0).int() == y_train).float())\n",
        "test_pred = model.forward(X_test)\n",
        "test_acc = torch.mean((custom_where(test_pred > 0.5, 1, 0).int() == y_test).float())\n",
        "print('Training Accuracy:    %.2f' % (train_acc*100))\n",
        "print('Test Accuracy:    %.2f' % (test_acc*100))\n",
        "print('Weights:   ', model.linear.weight)\n",
        "print('Bias:   ', model.linear.bias)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yEecLBup38zt"
      },
      "source": [
        "# Perceptron"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGsTlDeb38zu"
      },
      "source": [
        "5. Built a Perceptron model using nn.Module class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q_i75SVr38zu"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class Perceptron():\n",
        "    def __init__(self, num_features):\n",
        "        self.num_features = num_features\n",
        "        self.weights = torch.zeros(num_features, 1, \n",
        "                                   dtype=torch.float32, device=device)\n",
        "        self.bias = torch.zeros(1, dtype=torch.float32, device=device)\n",
        "        self.ones = torch.ones(1)\n",
        "        self.zeros = torch.zeros(1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        linear = torch.add(torch.mm(x, self.weights), self.bias)\n",
        "        predictions = custom_where(linear > 0., 1, 0).float()\n",
        "        return predictions\n",
        "        \n",
        "    def backward(self, x, y):  \n",
        "        predictions = self.forward(x)\n",
        "        errors = y - predictions\n",
        "        return errors\n",
        "        \n",
        "        \n",
        "    def train(self, x, y, epochs):\n",
        "        for e in range(epochs):\n",
        "            for i in range(y.shape[0]):\n",
        "                errors = self.backward(x[i].view(1, self.num_features), y[i]).view(-1)\n",
        "                self.weights += (errors * x[i]).view(self.num_features, 1)\n",
        "                self.bias += errors\n",
        "                \n",
        "    def evaluate(self, x, y):\n",
        "        predictions = self.forward(x).reshape(-1)\n",
        "        accuracy = torch.sum(predictions == y).float() / y.shape[0]\n",
        "        return accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O25kb6sy38zu"
      },
      "source": [
        "6. Load the 'perceptron_toydata' dataset\n",
        "\n",
        "    Split the dataset into train and test sets\n",
        "    \n",
        "    Normalize the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7PfbijT738zu",
        "outputId": "11604475-5f02-467c-fc55-abc29358c712"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Class label counts: tensor([50, 50])\n",
            "X.shape: torch.Size([100, 2])\n",
            "y.shape: torch.Size([100])\n",
            "torch.Size([70, 2])\n",
            "torch.Size([70])\n",
            "torch.Size([30, 2])\n",
            "torch.Size([30])\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "drive.mount('/content/drive')\n",
        "path='/content/drive/My Drive/deeplearning/perceptron_toydata.txt'\n",
        "\n",
        "df = pd.read_csv(path, index_col=None, header=None , delimiter='\\t')\n",
        "df.columns = ['x1', 'x2', 'y']\n",
        "X = torch.tensor(df[['x1', 'x2']].values, dtype=torch.float) \n",
        "y = torch.tensor(df['y'].values, dtype=torch.int) \n",
        "print('Class label counts:', torch.bincount(y))\n",
        "print('X.shape:', X.shape)\n",
        "print('y.shape:', y.shape)\n",
        "\n",
        "# Shuffling & train/test split\n",
        "shuffle_idx = torch.randperm(y.size(0), dtype=torch.long)\n",
        "X, y = X[shuffle_idx], y[shuffle_idx]\n",
        "percent70 = int(shuffle_idx.size(0)*0.7)\n",
        "X_train, X_test = X[shuffle_idx[:percent70]], X[shuffle_idx[percent70:]]\n",
        "y_train, y_test = y[shuffle_idx[:percent70]], y[shuffle_idx[percent70:]]\n",
        "# Normalize (mean zero, unit variance)\n",
        "mu, sigma = X_train.mean(axis=0), X_train.std(axis=0)\n",
        "X_train = (X_train - mu) / sigma\n",
        "X_test = (X_test - mu) / sigma\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyaaBEqe38zv"
      },
      "source": [
        "7. Train the perceptron"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qk9rrosE38zv",
        "outputId": "6ee44484-b494-490b-fa1b-6a524af7b6bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model parameters:\n",
            "  Weights: tensor([[2.4768],\n",
            "        [0.8986]])\n",
            "  Bias: tensor([-1.])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  after removing the cwd from sys.path.\n"
          ]
        }
      ],
      "source": [
        "model = Perceptron(num_features=2)\n",
        "\n",
        "Xtrain = torch.tensor(X_train, dtype=torch.float32, device=device)\n",
        "ytrain = torch.tensor(y_train, dtype=torch.float32, device=device)\n",
        "\n",
        "model.train(Xtrain, ytrain, epochs=5)\n",
        "\n",
        "print('Model parameters:')\n",
        "print('  Weights: %s' % model.weights)\n",
        "print('  Bias: %s' % model.bias)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1i_Fb3W38zv"
      },
      "source": [
        "8. evaluate the model (accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9QPvvopw38zv",
        "outputId": "a328264d-b643-4e83-ea44-929568d09a88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set accuracy: 100.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \n"
          ]
        }
      ],
      "source": [
        "Xtest = torch.tensor(X_test, dtype=torch.float32, device=device)\n",
        "ytest = torch.tensor(y_test, dtype=torch.float32, device=device)\n",
        "\n",
        "test_acc = model.evaluate(Xtest, ytest)\n",
        "print('Test set accuracy: %.2f%%' % (test_acc*100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUxVn-xQ38zw"
      },
      "source": [
        "# Multi Layer Perceptron"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcLd5fa238zw"
      },
      "source": [
        "Unlike the single-layer perceptron, the Multi Layer Perceptron models have hidden layers\n",
        "between the input and the output layers. After every hidden layer, an activation function \n",
        "is applied to introduce non-linearity. \n",
        "\n",
        "9. Built a simple Multi Layer Perceptron model withe one hidden layer. \n",
        "After the hidden layer, we will use ReLU as activation before the information is sent to the output layer.\n",
        "As an output activation function, we will use Sigmoid. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jEN9DkOb38zw"
      },
      "outputs": [],
      "source": [
        "class MultilayerPerceptron(torch.nn.Module):\n",
        "    def __init__(self, num_features,num_hidden_1):\n",
        "        super(MultilayerPerceptron, self).__init__()\n",
        "        self.linear_1 = torch.nn.Linear(num_features, num_hidden_1)\n",
        "        self.linear_out = torch.nn.Linear(num_hidden_1, 1)\n",
        "\n",
        "        \n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.linear_1(x)\n",
        "        out = F.relu(out)\n",
        "        out = self.linear_out(out)\n",
        "        out = F.sigmoid(out)\n",
        "        \n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhoV6HNj38zx"
      },
      "source": [
        "10. Create a random datasets and assign binary labels {0,1}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1BQXuRRi38zx",
        "outputId": "cc9ebfec-d7a9-45ef-dea7-70d4729ff2d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([40, 2])\n",
            "torch.Size([40])\n",
            "torch.Size([10, 2])\n",
            "torch.Size([10])\n"
          ]
        }
      ],
      "source": [
        "def blob_label(y, label, loc): # assign labels\n",
        "    target = np.copy(y)\n",
        "    for l in loc:\n",
        "        target[y == l] = label\n",
        "    return target\n",
        "x_train, y_train = make_blobs(n_samples=40, n_features=2, cluster_std=1.5, shuffle=True)\n",
        "x_train = torch.FloatTensor(x_train)\n",
        "y_train = torch.FloatTensor(blob_label(y_train, 0, [0]))\n",
        "y_train = torch.FloatTensor(blob_label(y_train, 1, [1,2,3]))\n",
        "x_test, y_test = make_blobs(n_samples=10, n_features=2, cluster_std=1.5, shuffle=True)\n",
        "x_test = torch.FloatTensor(x_test)\n",
        "y_test = torch.FloatTensor(blob_label(y_test, 0, [0]))\n",
        "y_test = torch.FloatTensor(blob_label(y_test, 1, [1,2,3]))\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "print(x_test.shape)\n",
        "print(y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flAKKykN38zx"
      },
      "source": [
        "11. Define the model with input dimension 2 and hidden dimension 10. \n",
        "Since the task is to classify binary labels, we can use BCELoss (Binary Cross Entropy Loss) as loss function.\n",
        "The optimizer is SGD (Stochastic Gradient Descent) with learning rate 0.01."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i0PMGWm638zx"
      },
      "outputs": [],
      "source": [
        "model = MultilayerPerceptron(2, 10)\n",
        "optimizer = torch.optim.SGD(model.parameters(),lr=0.01)  #use a SGD optimizer\n",
        "criterion = torch.nn.MSELoss() "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLK3PhI638zy"
      },
      "source": [
        "12. Check the test loss before the model training and compare it with the test loss after the training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CbN_E8dG38zy",
        "outputId": "452e0393-9cb4-44bc-dd4b-2bb3500fe4e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss before training 0.23654893040657043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "y_pred = model(x_test)\n",
        "before_train = criterion(y_pred.squeeze(), y_test)\n",
        "print('Test loss before training' , before_train.item())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.train()\n",
        "epoch = 20\n",
        "for epoch in range(epoch):\n",
        "    optimizer.zero_grad()\n",
        "    y_pred = model.forward(x_train) \n",
        "    loss = criterion(y_pred,y_train)\n",
        "    print('Epoch {}: train loss: {}'.format(epoch, loss.item()))\n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7MqTRtExbrc",
        "outputId": "43e3d15f-24c3-4467-e9e9-a61782d60d7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: train loss: 0.2857312858104706\n",
            "Epoch 1: train loss: 0.28241994976997375\n",
            "Epoch 2: train loss: 0.27909523248672485\n",
            "Epoch 3: train loss: 0.2757806181907654\n",
            "Epoch 4: train loss: 0.27250081300735474\n",
            "Epoch 5: train loss: 0.26928097009658813\n",
            "Epoch 6: train loss: 0.26614561676979065\n",
            "Epoch 7: train loss: 0.2631177306175232\n",
            "Epoch 8: train loss: 0.26021769642829895\n",
            "Epoch 9: train loss: 0.2574624717235565\n",
            "Epoch 10: train loss: 0.25486528873443604\n",
            "Epoch 11: train loss: 0.25243523716926575\n",
            "Epoch 12: train loss: 0.2501772344112396\n",
            "Epoch 13: train loss: 0.24809250235557556\n",
            "Epoch 14: train loss: 0.2461787760257721\n",
            "Epoch 15: train loss: 0.2444310337305069\n",
            "Epoch 16: train loss: 0.24284198880195618\n",
            "Epoch 17: train loss: 0.24140284955501556\n",
            "Epoch 18: train loss: 0.24010413885116577\n",
            "Epoch 19: train loss: 0.23894067108631134\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:520: UserWarning: Using a target size (torch.Size([40])) that is different to the input size (torch.Size([40, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model(x_test)\n",
        "before_train = criterion(y_pred.squeeze(), y_test)\n",
        "print('Test loss after training' , before_train.item())"
      ],
      "metadata": {
        "id": "okdQw_YY-Z5V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1oa4aGl38zy"
      },
      "source": [
        "13. In order to improve the model, you can try out different parameter values for your\n",
        "hyperparameters(ie. hidden dimension size, epoch size, learning rates). You can also \n",
        "try changing the structure of your model (ie. adding more hidden layers) to see if your\n",
        "mode improves. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fn5nvCxB38zy"
      },
      "outputs": [],
      "source": [
        "class MultilayerPerceptronDifferent(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, num_features,num_hidden_1,num_hidden_2):\n",
        "        super(MultilayerPerceptronDifferent, self).__init__()\n",
        "        self.linear_1 = torch.nn.Linear(num_features, num_hidden_1)\n",
        "        self.linear_2 = torch.nn.Linear(num_hidden_1, num_hidden_2)\n",
        "        self.linear_out = torch.nn.Linear(num_hidden_2, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.linear_1(x)\n",
        "        out = F.relu(out)\n",
        "        out = self.linear_2(out)\n",
        "        out = F.relu(out)\n",
        "        out = self.linear_out(out)\n",
        "        out = F.sigmoid(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "model1 = MultilayerPerceptronDifferent(2,512,256)\n",
        "optimizer = torch.optim.SGD(model.parameters(),lr=0.01) \n",
        "criterion = torch.nn.MSELoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ufqr70sm38zz",
        "outputId": "66e323a3-c6c0-431d-91fa-3fececd6a415"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: train loss: 0.23789472877979279\n",
            "Epoch 1: train loss: 0.23695605993270874\n",
            "Epoch 2: train loss: 0.2361147552728653\n",
            "Epoch 3: train loss: 0.23536136746406555\n",
            "Epoch 4: train loss: 0.2346871942281723\n",
            "Epoch 5: train loss: 0.2340841293334961\n",
            "Epoch 6: train loss: 0.23354469239711761\n",
            "Epoch 7: train loss: 0.23306213319301605\n",
            "Epoch 8: train loss: 0.2326304018497467\n",
            "Epoch 9: train loss: 0.23224399983882904\n",
            "Epoch 10: train loss: 0.2318979650735855\n",
            "Epoch 11: train loss: 0.23158787190914154\n",
            "Epoch 12: train loss: 0.2313098907470703\n",
            "Epoch 13: train loss: 0.23106049001216888\n",
            "Epoch 14: train loss: 0.23083651065826416\n",
            "Epoch 15: train loss: 0.23063524067401886\n",
            "Epoch 16: train loss: 0.2304542511701584\n",
            "Epoch 17: train loss: 0.23029132187366486\n",
            "Epoch 18: train loss: 0.23014454543590546\n",
            "Epoch 19: train loss: 0.2300122082233429\n",
            "Epoch 20: train loss: 0.22989274561405182\n",
            "Epoch 21: train loss: 0.22978487610816956\n",
            "Epoch 22: train loss: 0.22968733310699463\n",
            "Epoch 23: train loss: 0.22959907352924347\n",
            "Epoch 24: train loss: 0.22951915860176086\n",
            "Epoch 25: train loss: 0.22944670915603638\n",
            "Epoch 26: train loss: 0.2293809950351715\n",
            "Epoch 27: train loss: 0.22932127118110657\n",
            "Epoch 28: train loss: 0.22926704585552216\n",
            "Epoch 29: train loss: 0.22921770811080933\n",
            "Test loss after Training 0.23643937706947327\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:520: UserWarning: Using a target size (torch.Size([40])) that is different to the input size (torch.Size([40, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        }
      ],
      "source": [
        "model1.train()\n",
        "epoch = 30\n",
        "for epoch in range(epoch):\n",
        "    optimizer.zero_grad()\n",
        "    y_pred = model.forward(x_train) \n",
        "    loss = criterion(y_pred,y_train)\n",
        "    print('Epoch {}: train loss: {}'.format(epoch, loss.item()))\n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "model1.eval()\n",
        "y_pred = model1(x_test)\n",
        "after_train = criterion(y_pred.squeeze(), y_test) \n",
        "print('Test loss after Training' , after_train.item())"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "TP NÂ° 3 - DEEP LEARNING.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}